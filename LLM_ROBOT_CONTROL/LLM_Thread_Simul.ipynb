{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Example model IDs for illustration:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#   - 1) A \"language\" or \"chat\" transformer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#   - 2) An \"action\" or \"instruction\" transformer\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import threading\n",
    "\n",
    "# Example model IDs for illustration:\n",
    "#   - 1) A \"language\" or \"chat\" transformer\n",
    "#   - 2) An \"action\" or \"instruction\" transformer\n",
    "model_id_language = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id_action   = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Placeholder: use another model if desired\n",
    "\n",
    "# Create two separate pipelines\n",
    "pipeline_lang = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id_language,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipeline_action = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id_action,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Example messages (the same or different, depending on your use case)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\",   \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# Terminators can be pipeline-specific but we will just reuse for simplicity\n",
    "terminators = [\n",
    "    pipeline_lang.tokenizer.eos_token_id,\n",
    "    pipeline_lang.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "def generate_response(pipeline, prompt, terminators, result_container, index):\n",
    "    \"\"\"\n",
    "    Thread worker function that runs the pipeline and\n",
    "    stores the generated text in `result_container[index]`.\n",
    "    \"\"\"\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    # Store the entire text or just the appended portion; your choice:\n",
    "    result_container[index] = outputs[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Container for both results\n",
    "    results = [None, None]\n",
    "\n",
    "    # Create threads for parallel calls\n",
    "    t_lang = threading.Thread(\n",
    "        target=generate_response,\n",
    "        args=(pipeline_lang, messages, terminators, results, 0)\n",
    "    )\n",
    "    t_action = threading.Thread(\n",
    "        target=generate_response,\n",
    "        args=(pipeline_action, messages, terminators, results, 1)\n",
    "    )\n",
    "\n",
    "    # Start threads\n",
    "    t_lang.start()\n",
    "    t_action.start()\n",
    "\n",
    "    # Wait for both to complete\n",
    "    t_lang.join()\n",
    "    t_action.join()\n",
    "\n",
    "    # Now we have two outputs from two different pipelines\n",
    "    print(\"=== Language Transformer Output ===\")\n",
    "    print(results[0])\n",
    "    print(\"\\n=== Action Transformer Output ===\")\n",
    "    print(results[1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1) transformers 가 설치 안 되어 있으면 아래 주석 해제 후 실행\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# !pip install transformers\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m     10\u001b[0m     AutoTokenizer,\n\u001b[1;32m     11\u001b[0m     TextIteratorStreamer\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream_generation\u001b[39m(model, tokenizer, prompt, token_queue, done_event):\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    모델에 prompt를 넣어 실시간(스트리밍)으로 토큰을 생성한다.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    새로 생성된 토큰은 token_queue 에 넣는다.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    생성이 끝나면 done_event.set()으로 알린다.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "# 1) transformers 가 설치 안 되어 있으면 아래 주석 해제 후 실행\n",
    "# !pip install transformers\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TextIteratorStreamer\n",
    ")\n",
    "import torch\n",
    "\n",
    "def stream_generation(model, tokenizer, prompt, token_queue, done_event):\n",
    "    \"\"\"\n",
    "    모델에 prompt를 넣어 실시간(스트리밍)으로 토큰을 생성한다.\n",
    "    새로 생성된 토큰은 token_queue 에 넣는다.\n",
    "    생성이 끝나면 done_event.set()으로 알린다.\n",
    "    \"\"\"\n",
    "    # TextIteratorStreamer: 토큰 생성 시점마다 yield 해줌\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n",
    "    \n",
    "    # 텍스트를 텐서로 변환\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # generate()에 필요한 파라미터\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 50,  # 필요에 맞춰 조절\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"streamer\": streamer\n",
    "    }\n",
    "    generation_kwargs.update(inputs)\n",
    "\n",
    "    # 모델이 토큰을 계속 생성하는 부분을 별도 스레드에서 돌린다.\n",
    "    def _generate():\n",
    "        model.generate(**generation_kwargs)\n",
    "        done_event.set()  # 생성 종료를 알림\n",
    "\n",
    "    t = threading.Thread(target=_generate)\n",
    "    t.start()\n",
    "\n",
    "    # streamer 에서 새 토큰이 yield 될 때마다 queue에 넣는다.\n",
    "    for new_text in streamer:\n",
    "        token_queue.put(new_text)\n",
    "\n",
    "    t.join()  # 생성이 완전히 끝날 때까지 대기\n",
    "    done_event.set()\n",
    "\n",
    "def display_generation(role_name, token_queue, done_event):\n",
    "    \"\"\"\n",
    "    token_queue로 들어오는 토큰을 실시간으로 꺼내 출력한다.\n",
    "    done_event가 set 되면 종료한다.\n",
    "    \"\"\"\n",
    "    print(f\"=== {role_name} ===\", flush=True)\n",
    "    while not done_event.is_set() or not token_queue.empty():\n",
    "        try:\n",
    "            # 토큰이 들어올 때까지 0.1초 대기\n",
    "            new_text = token_queue.get(timeout=0.1)\n",
    "            print(new_text, end=\"\", flush=True)\n",
    "        except:\n",
    "            pass\n",
    "    print(\"\\n\")  # 한 줄 띄우고 종료\n",
    "\n",
    "def main():\n",
    "    # ModuleNotFoundError 에 대한 안내\n",
    "    print(\"만약 `ModuleNotFoundError: No module named 'transformers'` 오류가 나면:\")\n",
    "    print(\"!pip install transformers\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # (1) 모델/토크나이저 로드\n",
    "    # -----------------------------\n",
    "    # 인증이 필요 없는 공개 모델 2종을 사용합니다.\n",
    "    #  - model_lang_id: \"gpt2\"\n",
    "    #  - model_act_id : \"microsoft/DialoGPT-medium\"\n",
    "    model_lang_id = \"gpt2\"\n",
    "    model_act_id  = \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "    print(\"모델을 불러오는 중입니다... (CPU에서는 다소 시간이 걸릴 수 있음)\\n\")\n",
    "    model_lang = AutoModelForCausalLM.from_pretrained(model_lang_id)\n",
    "    tokenizer_lang = AutoTokenizer.from_pretrained(model_lang_id)\n",
    "\n",
    "    model_act = AutoModelForCausalLM.from_pretrained(model_act_id)\n",
    "    tokenizer_act = AutoTokenizer.from_pretrained(model_act_id)\n",
    "\n",
    "    # GPU가 있으면 GPU 사용 (없으면 CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_lang.to(device)\n",
    "    model_act.to(device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # (2) 사용자 프롬프트 설정\n",
    "    # -----------------------------\n",
    "    # 간단한 예시로 \"Who are you?\" 를 사용.\n",
    "    prompt = \"Who are you?\\n\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # (3) 병렬 스레드로 스트리밍 출력\n",
    "    # -----------------------------\n",
    "    # 각 모델별로 queue / event를 만들어준다\n",
    "    queue_lang = queue.Queue()\n",
    "    queue_act  = queue.Queue()\n",
    "\n",
    "    done_event_lang = threading.Event()\n",
    "    done_event_act  = threading.Event()\n",
    "\n",
    "    # generate + streamer 동작 스레드\n",
    "    t_gen_lang = threading.Thread(target=stream_generation,\n",
    "                                  args=(model_lang, tokenizer_lang, prompt, queue_lang, done_event_lang))\n",
    "    t_gen_act  = threading.Thread(target=stream_generation,\n",
    "                                  args=(model_act, tokenizer_act, prompt, queue_act, done_event_act))\n",
    "\n",
    "    # 실시간 출력 스레드\n",
    "    t_disp_lang = threading.Thread(target=display_generation, \n",
    "                                   args=(\"Language Transformer\", queue_lang, done_event_lang))\n",
    "    t_disp_act  = threading.Thread(target=display_generation, \n",
    "                                   args=(\"Action Transformer\", queue_act, done_event_act))\n",
    "\n",
    "    print(\"생성을 시작합니다...\\n\")\n",
    "\n",
    "    # 스레드 시작\n",
    "    t_gen_lang.start()\n",
    "    t_gen_act.start()\n",
    "    t_disp_lang.start()\n",
    "    t_disp_act.start()\n",
    "\n",
    "    # 모든 스레드가 끝날 때까지 대기\n",
    "    t_gen_lang.join()\n",
    "    t_gen_act.join()\n",
    "    t_disp_lang.join()\n",
    "    t_disp_act.join()\n",
    "\n",
    "    print(\"=== 모든 생성이 완료되었습니다. ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp39-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from torch) (2024.12.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp39-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, torch\n",
      "Successfully installed mpmath-1.3.0 sympy-1.13.1 torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (5.29.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델/토크나이저 로딩 중... 시간이 다소 걸릴 수 있습니다. ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/Users/jeongsugwang/.pyenv/versions/3.9.13/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델/토크나이저 로딩 완료! ===\n",
      "\n",
      "=== 두 모델에서 동시에 스트리밍 답변을 시작합니다. ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongsugwang/.pyenv/versions/3.9.13/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[언어출력] [언어출력] 한국 [언어출력] [언어출력] 내에서도 [언어출력] 널리 [언어출력] 알려진 [언어출력] 한국의 [언어출력] [언어출력] 수도와 [언어출력] 간단한 [언어출력] 역사적 [언어출력] 배경을 [언어출력] [언어출력] 알려주고,\n",
      "[언어출력] [언어출력] [언어출력] [언어출력] 답변: [언어출력] 한국의 [언어출력] [언어출력] 수도와 [언어출력] 주요 [언어출력] 역사적 [언어출력] 배경을 [언어출력] [언어출력] 알려주고,\n",
      "[언어출력] [언어출력] [언어출력] [언어출력] 답변: [언어출력] 한국 [언어출력] [언어출력] 내에서도 [언어출력] 널리 [언어출력] 알려진 [언어출력] 한국 [언어출력] [언어출력] 수도와 [언어출력] 주요 [언어출력] 역사적 [언어출력] 배경을 [언어출력] [언어출력] 알려주고,\n",
      "[언어출력] [언어출력] [언어출력] [언어출력] 답변: [언어출력] 한국의 [언어출력] [언어출력] 수도와 [언어출력] 주요 [언어출력] 역사적 [언어출력] 배경을 [언어출력] [언어출력] 알려주고,\n",
      "[언어출력] [언어출력] [언어출력] [언어출력] 답변: [언어출력] 한국 [언어출력] [언어출력] 내에서도 [언어출력] 널리 [언어출력] 알려진 [언어출력] 한국 [언어출력] [언어출력] 수도와 [언어출력] 주요 [언어출력] 역사적 [언어출력] 배경을 [언어출력] [언어출력] 알려주고,\n",
      "[언어출력] [언어출력] [언어출력] [언어출력] 답변: [언어출력] 한국 [언어출력] [언어출력] 내에서도 [언어출력] 널리 [언어출력] 알려진 [언어출력] 한국 [언어출력] [언어출력] 수도와 [언어출력] 주요 [언어출력] 역사적 [언어출력] 배경을 [언어출력] [언어출력] 알려주고,\n",
      "[언어출력] [언어출력] [언어출력] [언어출력] 답변: [언어출력] 한국의 [언어출력] [언어출력] 수도와 [언어출력] 주요 [언어출력] 역사적 [언어출력] 배경을 [언어출력] [언어출력] 알려주고,\n",
      "[언어출력] [언어출력] [언어출력] [언어출력] 답변: [언어출력] 한국 [언어출력] [언어출력] 내에서도 [언어출력] 널리 [언어출력] 알려진 [언어출력] 한국 [언어출력] [언어출력] 수도와 [언어출력] 주요 [언어출력] 역사적 [언어출력] 배경을 [언어출력] [언어출력] 알려주고,\n",
      "[언어출력] [언어출력] [언어출력] [언어출력] 답변: [언어출력] 한국의 [언어출력] [언어출력] 수도와 [언어출력] 주요 [언어출력] 역사적 [언어출력] 배경을 [언어출력] 알려[행동출력] [행동출력] [행동출력] 서울이 [행동출력] 아니라\n",
      "[행동출력] [행동출력] [행동출력] 목적: [행동출력] 대한민국의 [행동출력] [행동출력] 수도와 [행동출력] 간단한 [행동출력] 역사적 [행동출력] 배경을 [행동출력] [행동출력] 알려주고,\n",
      "[행동출력] [행동출력] 이 [행동출력] 내용에 [행동출력] [행동출력] 기반해 [행동출력] 오늘 [행동출력] 할 [행동출력] 수 [행동출력] 있는 [행동출력] [행동출력] [행동출력] [행동출력] [행동출력] 액티비티를 [행동출력] [행동출력] 추천해.\n",
      "[행동출력] [행동출력] [행동출력] [행동출력] 행운: [행동출력] [행동출력] 성공, [행동출력] [행동출력] 실패, [행동출력] [행동출력] 실패, [행동출력] [행동출력] 실패, [행동출력] [행동출력] 실패, [행동출력] 실패\n",
      "[행동출력] [행동출력] [행동출력] 효과: [행동출력] [행동출력] 성공, [행동출력] [행동출력] 실패, [행동출력] [행동출력] 실패, [행동출력] [행동출력] 실패, [행동출력] [행동출력] 실패, [행동출력] [행동출력] 실패, [행동출력] [행동출력] 실패, [행동출력] 실패\n",
      "[행동출력] [행동출력] 행동 [행동출력] [행동출력] 계획: [행동출력] 서울의 [행동출력] [행동출력] 수도와 [행동출력] 관련된 [행동출력] 자세한 [행동출력] [행동출력] 설명과 [행동출력] 함께\n",
      "[행동출력] [행동출력] 행동 [행동출력] [행동출력] 계획: [행동출력] 서울을 [행동출력] 비롯한 [행동출력] [행동출력] 수도권과 [행동출력] 관련된 [행동출력] 구체적인 [행동출력] [행동출력] 소개와 [행동출력] 함께\n",
      "[행동출력] [행동출력] [행동출력] 목적: [행동출력] 서울의 [행동출력] [행동출력] 수도와 [행동출력] 관련된 [행동출력] 자세한 [행동출력] [행동출력] 설명과 [행동출력] 함께\n",
      "[행동출력] [행동출력] 행동 [행동출력] [행동출력] 계획: [행동출력] [행동출력] 서울이 [행동출력] 아니라\n",
      "[행동출력] [행동출력] [행동출력] 목적: [행동출력] 서울을 [행동출력] 포함한 [행동출력] [행동출력] 수도권의 [행동출력] 역사, [행동출력] 주요 [행동출력] [행동출력] 인물, [행동출력] 사건, [행동출력] 사건, [행동출력] 사건 [행동출력] 등을 [행동출력] [행동출력] [행동출력] 소개해줘.\n",
      "[행동출력] [행동출력] 행동 [행동출력] [행동출력] 계획:\n",
      "\n",
      "=== 최종 결과 정리 ===\n",
      "\n",
      "[언어적 출력]\n",
      " 한국 내에서도 널리 알려진 한국의 수도와 간단한 역사적 배경을 알려주고,\n",
      "답변: 한국의 수도와 주요 역사적 배경을 알려주고,\n",
      "답변: 한국 내에서도 널리 알려진 한국 수도와 주요 역사적 배경을 알려주고,\n",
      "답변: 한국의 수도와 주요 역사적 배경을 알려주고,\n",
      "답변: 한국 내에서도 널리 알려진 한국 수도와 주요 역사적 배경을 알려주고,\n",
      "답변: 한국 내에서도 널리 알려진 한국 수도와 주요 역사적 배경을 알려주고,\n",
      "답변: 한국의 수도와 주요 역사적 배경을 알려주고,\n",
      "답변: 한국 내에서도 널리 알려진 한국 수도와 주요 역사적 배경을 알려주고,\n",
      "답변: 한국의 수도와 주요 역사적 배경을 알려 \n",
      "\n",
      "[행동적 출력]\n",
      " 서울이 아니라\n",
      "목적: 대한민국의 수도와 간단한 역사적 배경을 알려주고,\n",
      "이 내용에 기반해 오늘 할 수 있는 액티비티를 추천해.\n",
      "행운: 성공, 실패, 실패, 실패, 실패, 실패\n",
      "효과: 성공, 실패, 실패, 실패, 실패, 실패, 실패, 실패\n",
      "행동 계획: 서울의 수도와 관련된 자세한 설명과 함께\n",
      "행동 계획: 서울을 비롯한 수도권과 관련된 구체적인 소개와 함께\n",
      "목적: 서울의 수도와 관련된 자세한 설명과 함께\n",
      "행동 계획: 서울이 아니라\n",
      "목적: 서울을 포함한 수도권의 역사, 주요 인물, 사건, 사건, 사건 등을 소개해줘.\n",
      "행동 계획: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "import queue\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    TextIteratorStreamer,\n",
    ")\n",
    "\n",
    "#############################################\n",
    "# 1) 두 개의 다른 모델(혹은 동일 모델)을 준비\n",
    "#############################################\n",
    "\n",
    "# 여기서는 예시로 동일한 skt/kogpt2-base-v2 모델을\n",
    "# \"언어적 출력용\" / \"행동적 출력용\" 두 번 로드한다고 가정합니다.\n",
    "\n",
    "MODEL_NAME_1 = \"skt/kogpt2-base-v2\"  # 언어적 출력용\n",
    "MODEL_NAME_2 = \"skt/kogpt2-base-v2\"  # 행동적 출력용 (예시상 동일 모델)\n",
    "\n",
    "# (필요시) 캐시 경로 설정\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\"\n",
    "\n",
    "print(\"=== 모델/토크나이저 로딩 중... 시간이 다소 걸릴 수 있습니다. ===\")\n",
    "\n",
    "# 토크나이저 / 모델 로드 (모델1)\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(MODEL_NAME_1)\n",
    "model_1 = AutoModelForCausalLM.from_pretrained(MODEL_NAME_1)\n",
    "streamer_1 = TextIteratorStreamer(tokenizer_1, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "pipeline_1 = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model_1,\n",
    "    tokenizer=tokenizer_1,\n",
    "    device=-1,            # CPU 사용: -1 / GPU 사용: 0\n",
    "    max_new_tokens=128,   # 생성할 토큰 최대 개수\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    streamer=streamer_1,  # <-- 스트리머 장착 (실시간 토큰 생성)\n",
    ")\n",
    "\n",
    "# 토크나이저 / 모델 로드 (모델2)\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(MODEL_NAME_2)\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(MODEL_NAME_2)\n",
    "streamer_2 = TextIteratorStreamer(tokenizer_2, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "pipeline_2 = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model_2,\n",
    "    tokenizer=tokenizer_2,\n",
    "    device=-1,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    streamer=streamer_2,\n",
    ")\n",
    "\n",
    "print(\"=== 모델/토크나이저 로딩 완료! ===\\n\")\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 2) 스레드에서 모델 추론 + 스트리밍 출력\n",
    "#############################################\n",
    "\n",
    "def generate_stream(\n",
    "    pipe, \n",
    "    prompt: str, \n",
    "    streamer: TextIteratorStreamer, \n",
    "    output_queue: queue.Queue,\n",
    "    prefix_name: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    특정 pipeline(모델)에 prompt를 넣고,\n",
    "    스트리밍으로 토큰이 생성될 때마다 print로 찍어주는 함수.\n",
    "    최종 출력은 output_queue에 put()하여 반환.\n",
    "    \"\"\"\n",
    "    # 1) prompt를 주어 추론 시작 (비동기적으로 토큰 생성)\n",
    "    _ = pipe(prompt)\n",
    "\n",
    "    # 2) 스트리머에서 토큰을 하나씩 뽑아가며 바로 print\n",
    "    text_buffer = \"\"\n",
    "    for new_token in streamer:\n",
    "        text_buffer += new_token\n",
    "        print(f\"{prefix_name}{new_token}\", end=\"\", flush=True)\n",
    "\n",
    "    # 3) 최종 결과 문자열을 Queue에 넣음\n",
    "    output_queue.put(text_buffer)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 예시 입력(하나)\n",
    "    user_input = \"대한민국의 수도와 간단한 역사적 배경을 알려주고,\\n\" \\\n",
    "                 \"이 내용에 기반해 오늘 할 수 있는 액티비티를 추천해줘.\"\n",
    "\n",
    "    # 1) 언어적 출력용 프롬프트\n",
    "    prompt_for_language = (\n",
    "        \"다음 질문에 대해 친절하고 자세한 한국어 답변을 작성해줘:\\n\\n\"\n",
    "        f\"질문: {user_input}\\n\\n답변:\"\n",
    "    )\n",
    "    # 2) 행동(액션) 출력용 프롬프트\n",
    "    prompt_for_action = (\n",
    "        \"당신은 여행 가이드처럼 행동해야 합니다.\\n\"\n",
    "        \"질문을 보고, 구체적인 액션 플랜을 단계별로 제시하세요.\\n\\n\"\n",
    "        f\"질문: {user_input}\\n\\n행동 계획:\"\n",
    "    )\n",
    "\n",
    "    # 두 개의 Queue: 각 스레드(모델)에서 최종 text를 담아놓을 곳\n",
    "    output_queue_1 = queue.Queue()\n",
    "    output_queue_2 = queue.Queue()\n",
    "\n",
    "    print(\"=== 두 모델에서 동시에 스트리밍 답변을 시작합니다. ===\\n\")\n",
    "\n",
    "    # 스레드 생성\n",
    "    t1 = threading.Thread(\n",
    "        target=generate_stream,\n",
    "        args=(pipeline_1, prompt_for_language, streamer_1, output_queue_1, \"[언어출력] \")\n",
    "    )\n",
    "    t2 = threading.Thread(\n",
    "        target=generate_stream,\n",
    "        args=(pipeline_2, prompt_for_action, streamer_2, output_queue_2, \"[행동출력] \")\n",
    "    )\n",
    "\n",
    "    # 스레드 시작\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # 두 스레드 종료 대기\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    # 최종 결과 받기\n",
    "    lang_result = output_queue_1.get()\n",
    "    action_result = output_queue_2.get()\n",
    "\n",
    "    print(\"\\n\\n=== 최종 결과 정리 ===\\n\")\n",
    "    print(\"[언어적 출력]\\n\", lang_result, \"\\n\")\n",
    "    print(\"[행동적 출력]\\n\", action_result, \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama.list() 반환 값: models=[Model(model='llama3.2:latest', modified_at=datetime.datetime(2024, 10, 9, 2, 3, 50, 841424, tzinfo=TzInfo(+09:00)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M'))]\n",
      "모델 목록을 가져오는 중 오류가 발생했습니다: 0\n",
      "\n",
      "[언어적 출력 시작]\n",
      "\n",
      "[행동적 출력 시작]\n",
      "\n",
      "\n",
      "[언어적 출력 오류]: model \"gemma\" not found, try pulling it first\n",
      "\n",
      "[행동적 출력 오류]: model \"mistral\" not found, try pulling it first\n",
      "\n",
      "\n",
      "모든 스트리밍이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import ollama\n",
    "\n",
    "def pull_models():\n",
    "    \"\"\"\n",
    "    모델을 Pull하여 설치하는 함수\n",
    "    \"\"\"\n",
    "    models_to_pull = [\"gemma\", \"mistral\"]\n",
    "    try:\n",
    "        models_info = ollama.list()\n",
    "        print(f\"ollama.list() 반환 값: {models_info}\")  # 디버깅용 출력\n",
    "\n",
    "        # 'models_info'가 'models'라는 속성을 가지고 있다고 가정\n",
    "        models_list = models_info.models\n",
    "\n",
    "        # 모델 리스트가 Model 객체의 리스트라고 가정\n",
    "        installed_models = [model.model for model in models_list]\n",
    "\n",
    "        for model in models_to_pull:\n",
    "            if model not in installed_models:\n",
    "                print(f\"모델 '{model}'을(를) Pull 중입니다...\")\n",
    "                try:\n",
    "                    ollama.pull(model)\n",
    "                    print(f\"모델 '{model}'이(가) 성공적으로 설치되었습니다.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"모델 '{model}'을(를) Pull하는 중 오류가 발생했습니다: {e}\")\n",
    "            else:\n",
    "                print(f\"모델 '{model}'은(는) 이미 설치되어 있습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"모델 목록을 가져오는 중 오류가 발생했습니다: {e}\")\n",
    "\n",
    "def stream_language_output(user_prompt):\n",
    "    \"\"\"\n",
    "    첫 번째 출력(언어적 표현)에 해당하는 모델 스트리밍 함수\n",
    "    \"\"\"\n",
    "    print(\"\\n[언어적 출력 시작]\\n\")\n",
    "    try:\n",
    "        for chunk in ollama.generate(\n",
    "            model=\"gemma\",  # 실제 사용하실 모델명\n",
    "            prompt=user_prompt,\n",
    "            stream=True,\n",
    "            system=\"\"\"\n",
    "            너는 한국어로 답변을 작성하는 '언어적' 모델이야.\n",
    "            \"\"\",\n",
    "        ):\n",
    "            # chunk는 {\"completion\": \"...\"} 같은 형태로 토큰이 생성\n",
    "            print(chunk[\"completion\"], end=\"\", flush=True)\n",
    "        print(\"\\n[언어적 출력 종료]\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"[언어적 출력 오류]: {e}\\n\")\n",
    "\n",
    "def stream_action_output(user_prompt):\n",
    "    \"\"\"\n",
    "    두 번째 출력(행동적 표현)에 해당하는 모델 스트리밍 함수\n",
    "    \"\"\"\n",
    "    print(\"\\n[행동적 출력 시작]\\n\")\n",
    "    try:\n",
    "        for chunk in ollama.generate(\n",
    "            model=\"mistral\",  # 실제 사용하실 모델명\n",
    "            prompt=user_prompt,\n",
    "            stream=True,\n",
    "            system=\"\"\"\n",
    "            너는 특정 행동이나 단계(step-by-step action)를 설명하는 '행동적' 모델이야.\n",
    "            \"\"\",\n",
    "        ):\n",
    "            print(chunk[\"completion\"], end=\"\", flush=True)\n",
    "        print(\"\\n[행동적 출력 종료]\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"[행동적 출력 오류]: {e}\\n\")\n",
    "\n",
    "def main():\n",
    "    # 모델 Pull 및 설치\n",
    "    pull_models()\n",
    "\n",
    "    user_input = input(\"사용자 입력을 작성해주세요: \")\n",
    "\n",
    "    # 두 개의 스레드를 각각 실행\n",
    "    t1 = threading.Thread(target=stream_language_output, args=(user_input,))\n",
    "    t2 = threading.Thread(target=stream_action_output, args=(user_input,))\n",
    "\n",
    "    # 스레드 시작\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # 두 스레드가 모두 끝날 때까지 대기\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    print(\"\\n모든 스트리밍이 완료되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpull\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllama2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/ollama/_client.py:421\u001b[0m, in \u001b[0;36mClient.pull\u001b[0;34m(self, model, insecure, stream)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpull\u001b[39m(\n\u001b[1;32m    410\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    411\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ProgressResponse, Iterator[ProgressResponse]]:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m  Raises `ResponseError` if the request could not be fulfilled.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m  Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mProgressResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/pull\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPullRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m      \u001b[49m\u001b[43minsecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minsecure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/ollama/_client.py:177\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/ollama/_client.py:118\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 118\u001b[0m   r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpx/_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    836\u001b[0m )\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull('llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='llama3.2:latest', modified_at=datetime.datetime(2024, 10, 9, 2, 3, 50, 841424, tzinfo=TzInfo(+09:00)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. He discovered that shorter wavelengths of light are scattered more by tiny molecules in the atmosphere than longer wavelengths.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. When sunlight enters Earth's atmosphere, it encounters tiny molecules like nitrogen (N2) and oxygen (O2).\n",
      "2. These molecules scatter the light in all directions, but they scatter shorter wavelengths (like blue and violet) more than longer wavelengths (like red and orange).\n",
      "3. The scattered blue light is then dispersed throughout the sky, giving it a blue appearance.\n",
      "4. Our eyes perceive this scattering of blue light as the color of the sky.\n",
      "\n",
      "Now, you might be wondering why the sky isn't just a deep shade of purple, since violet light is scattered even more than blue light. That's because our eyes are more sensitive to blue light, and the human eye has a built-in bias towards perceiving blues and greens rather than purples.\n",
      "\n",
      "Additionally, other factors can affect the color of the sky, such as:\n",
      "\n",
      "* Dust, water vapor, and pollutants in the atmosphere, which can scatter light and change its apparent color.\n",
      "* The angle of the sun, which can create beautiful sunsets and sunrises with hues of orange, pink, and red.\n",
      "* Time of day, which can also affect the intensity and color of the light.\n",
      "\n",
      "So, there you have it! The sky appears blue due to Rayleigh scattering and our eyes' sensitivity to shorter wavelengths of light.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='llama3.2:latest', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. He discovered that when sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. Sunlight is made up of different colors, like a rainbow.\n",
      "2. When sunlight enters the atmosphere, these colors are scattered in all directions by the tiny molecules.\n",
      "3. The shorter (blue) wavelengths are scattered more than the longer (red) wavelengths because they are more easily deflected by the smaller molecules.\n",
      "\n",
      "Think of it like this: imagine you're at the beach and throw a blue ball into the air. What happens? It scatters in all directions, right? That's similar to what's happening with sunlight and its colors when it enters our atmosphere!\n",
      "\n",
      "As a result of this scattering, the blue light is dispersed throughout the sky, making it appear blue to our eyes. The color we see can also change depending on the time of day, atmospheric conditions, and other factors.\n",
      "\n",
      "Here are some additional interesting facts:\n",
      "\n",
      "* During sunrise and sunset, the light has to travel through more of the atmosphere, which scatters out even more blue light, leaving mainly red and orange wavelengths to reach our eyes.\n",
      "* The color of the sky can vary depending on atmospheric conditions, such as pollution or dust particles, which can scatter light in different ways.\n",
      "\n",
      "So, that's why the sky is blue!"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def handle_streaming_response(response_stream):\n",
    "    for chunk in response_stream:\n",
    "        # 각 청크를 디코딩하여 출력\n",
    "        if 'content' in chunk['message']:\n",
    "            print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "# 채팅 요청을 스트리밍 모드로 설정\n",
    "response = ollama.chat(\n",
    "    model='llama3.2:latest',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Why is the sky blue?',\n",
    "        },\n",
    "    ],\n",
    "    stream=True  # 스트리밍 활성화\n",
    ")\n",
    "\n",
    "# 스트리밍 응답 처리\n",
    "handle_streaming_response(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행동적 답변: Here언어적 답변: What행동적 답변:  are언어적 답변:  a행동적 답변:  some언어적 답변:  de행동적 답변:  actionable언어적 답변: cept행동적 답변:  steps언어적 답변: ively행동적 답변:  to언어적 답변:  simple행동적 답변:  explore언어적 답변:  question행동적 답변:  why언어적 답변: !행동적 답변:  the언어적 답변:  The행동적 답변:  sky언어적 답변:  inquiry행동적 답변:  appears언어적 답변:  \"행동적 답변:  blue언어적 답변: Why행동적 답변: :\n",
      "\n",
      "언어적 답변:  is행동적 답변: **언어적 답변:  the행동적 답변: Step언어적 답변:  sky행동적 답변:  언어적 답변:  blue행동적 답변: 1언어적 답변: ?\"행동적 답변: :언어적 답변:  del행동적 답변:  Ob언어적 답변: ves행동적 답변: serve언어적 답변:  into행동적 답변:  and언어적 답변:  the행동적 답변:  Note언어적 답변:  realm행동적 답변:  Your언어적 답변:  of행동적 답변:  Surround언어적 답변:  physics행동적 답변: ings언어적 답변: ,행동적 답변: **\n",
      "\n",
      "언어적 답변:  atmospheric행동적 답변: *언어적 답변:  science행동적 답변:  Find언어적 답변: ,행동적 답변:  a언어적 답변:  and행동적 답변:  comfortable언어적 답변:  lingu행동적 답변:  spot언어적 답변: istics행동적 답변:  outside언어적 답변: .행동적 답변:  with언어적 답변:  Let행동적 답변:  minimal언어적 답변: 's행동적 답변:  ob언어적 답변:  dissect행동적 답변: structions언어적 답변:  this행동적 답변:  (언어적 답변:  query행동적 답변: e언어적 답변:  through행동적 답변: .g언어적 답변:  a행동적 답변: .,언어적 답변:  multi행동적 답변:  trees언어적 답변: -f행동적 답변: ,언어적 답변: ac행동적 답변:  buildings언어적 답변: eted행동적 답변: ).\n",
      "언어적 답변:  lens행동적 답변: *언어적 답변: :\n",
      "\n",
      "행동적 답변:  Look언어적 답변: 1행동적 답변:  up언어적 답변: .행동적 답변:  at언어적 답변:  **행동적 답변:  the언어적 답변: Semantic행동적 답변:  sky언어적 답변:  Analysis행동적 답변:  on언어적 답변: **:행동적 답변:  a언어적 답변:  The행동적 답변:  clear언어적 답변:  word행동적 답변:  day언어적 답변:  \"행동적 답변:  when언어적 답변: blue행동적 답변:  it언어적 답변: \"행동적 답변: 's언어적 답변:  has행동적 답변:  not언어적 답변:  a행동적 답변:  too언어적 답변:  meaning행동적 답변:  cloudy언어적 답변:  that행동적 답변: .\n",
      "언어적 답변:  con행동적 답변: *언어적 답변: notes행동적 답변:  Take언어적 답변:  a행동적 답변:  note언어적 답변:  specific행동적 답변:  of언어적 답변:  wavelength행동적 답변:  the언어적 답변:  range행동적 답변:  color언어적 답변:  of행동적 답변:  of언어적 답변:  light행동적 답변:  the언어적 답변:  (행동적 답변:  sky언어적 답변: approximately행동적 답변: .언어적 답변:  행동적 답변:  Is언어적 답변: 450행동적 답변:  it언어적 답변: -행동적 답변:  really언어적 답변: 495행동적 답변:  blue언어적 답변:  nan행동적 답변: ?언어적 답변: ometers행동적 답변:  Are언어적 답변: )행동적 답변:  there언어적 답변:  associated행동적 답변:  any언어적 답변:  with행동적 답변:  shades언어적 답변:  the행동적 답변:  of언어적 답변:  color행동적 답변:  gray언어적 답변:  we행동적 답변:  or언어적 답변:  perceive행동적 답변:  yellow언어적 답변:  as행동적 답변: ?\n",
      "\n",
      "언어적 답변:  blue행동적 답변: **언어적 답변: .행동적 답변: Step언어적 답변:  However행동적 답변:  언어적 답변: ,행동적 답변: 2언어적 답변:  the행동적 답변: :언어적 답변:  term행동적 답변:  Research언어적 답변:  \"행동적 답변:  and언어적 답변: blue행동적 답변:  Learn언어적 답변: \"행동적 답변: **\n",
      "\n",
      "언어적 답변:  can행동적 답변: *언어적 답변:  also행동적 답변:  Start언어적 답변:  be행동적 답변:  by언어적 답변:  used행동적 답변:  reading언어적 답변:  to행동적 답변:  about언어적 답변:  describe행동적 답변:  the언어적 답변:  various행동적 답변:  science언어적 답변:  shades행동적 답변:  behind언어적 답변:  and행동적 답변:  why언어적 답변:  nuances행동적 답변:  the언어적 답변:  of행동적 답변:  sky언어적 답변:  this행동적 답변:  appears언어적 답변:  wavelength행동적 답변:  blue언어적 답변: .행동적 답변: .언어적 답변:  In행동적 답변:  You언어적 답변:  lingu행동적 답변:  can언어적 답변: istics행동적 답변:  find언어적 답변: ,행동적 답변:  many언어적 답변:  words행동적 답변:  online언어적 답변:  like행동적 답변:  resources언어적 답변:  \"행동적 답변: ,언어적 답변: blue행동적 답변:  including언어적 답변: \"행동적 답변:  articles언어적 답변:  belong행동적 답변:  and언어적 답변:  to행동적 답변:  videos언어적 답변:  the행동적 답변:  from언어적 답변:  category행동적 답변:  reputable언어적 답변:  of행동적 답변:  sources언어적 답변:  color행동적 답변:  like언어적 답변:  terms행동적 답변:  NASA언어적 답변: ,행동적 답변: ,언어적 답변:  which행동적 답변:  National언어적 답변:  are행동적 답변:  Geographic언어적 답변:  complex행동적 답변: ,언어적 답변: ,행동적 답변:  or언어적 답변:  multid행동적 답변:  Science언어적 답변: imensional행동적 답변: .com언어적 답변:  concepts행동적 답변: .\n",
      "언어적 답변:  that행동적 답변: *언어적 답변:  don행동적 답변:  Watch언어적 답변: 't행동적 답변:  educational언어적 답변:  always행동적 답변:  videos언어적 답변:  map행동적 답변:  on언어적 답변:  directly행동적 답변:  YouTube언어적 답변:  onto행동적 답변:  or언어적 답변:  objective행동적 답변:  Crash언어적 답변:  physical행동적 답변:  Course언어적 답변:  properties행동적 답변:  that언어적 답변: .\n",
      "\n",
      "행동적 답변:  explain언어적 답변: 2행동적 답변:  the언어적 답변: .행동적 답변:  concept언어적 답변:  **행동적 답변:  of언어적 답변: Lex행동적 답변:  light언어적 답변: ical행동적 답변:  scattering언어적 답변: ization행동적 답변:  and언어적 답변: **:행동적 답변:  Ray언어적 답변:  The행동적 답변: leigh언어적 답변:  word행동적 답변:  scattering언어적 답변:  \"행동적 답변: .\n",
      "\n",
      "언어적 답변: blue행동적 답변: **언어적 답변: \"행동적 답변: Step언어적 답변:  is행동적 답변:  언어적 답변:  a행동적 답변: 3언어적 답변:  lexical행동적 답변: :언어적 답변:  item행동적 답변:  Conduct언어적 답변:  in행동적 답변:  a언어적 답변:  English행동적 답변:  Simple언어적 답변: ,행동적 답변:  Experiment언어적 답변:  with행동적 답변: **\n",
      "\n",
      "언어적 답변:  its행동적 답변: *언어적 답변:  origin행동적 답변:  Get언어적 답변:  rooted행동적 답변:  some언어적 답변:  in행동적 답변:  colored언어적 답변:  Old행동적 답변:  scar언어적 답변:  English행동적 답변: ves언어적 답변:  and행동적 답변: ,언어적 답변:  Middle행동적 답변:  markers언어적 답변:  English행동적 답변: ,언어적 답변: .행동적 답변:  or언어적 답변:  It행동적 답변:  paint언어적 답변:  has행동적 답변:  to언어적 답변:  undergone행동적 답변:  create언어적 답변:  changes행동적 답변:  a언어적 답변:  over행동적 답변:  homemade언어적 답변:  time행동적 답변:  \"언어적 답변:  due행동적 답변: light언어적 답변:  to행동적 답변:  scattering언어적 답변:  linguistic행동적 답변: \"언어적 답변:  evolution행동적 답변:  experiment언어적 답변: ,행동적 답변: .\n",
      "언어적 답변:  borrowing행동적 답변: *언어적 답변:  from행동적 답변:  Fill언어적 답변:  other행동적 답변:  a언어적 답변:  languages행동적 답변:  container언어적 답변:  (행동적 답변:  with언어적 답변: e행동적 답변:  water언어적 답변: .g행동적 답변:  and언어적 답변: .,행동적 답변:  add언어적 답변:  French행동적 답변:  a언어적 답변:  ble행동적 답변:  few언어적 답변: u행동적 답변:  drops언어적 답변: ),행동적 답변:  of언어적 답변:  and행동적 답변:  blue언어적 답변:  context행동적 답변:  food언어적 답변: -dependent행동적 답변:  coloring언어적 답변:  usage행동적 답변: .언어적 답변: .행동적 답변:  Stir언어적 답변:  This행동적 답변:  it언어적 답변:  highlights행동적 답변:  gently언어적 답변:  the행동적 답변:  to언어적 답변:  dynamic행동적 답변:  distribute언어적 답변:  nature행동적 답변:  the언어적 답변:  of행동적 답변:  color언어적 답변:  language행동적 답변:  evenly언어적 답변: ,행동적 답변: .\n",
      "언어적 답변:  where행동적 답변: *언어적 답변:  words행동적 답변:  Now언어적 답변:  can행동적 답변: ,언어적 답변:  take행동적 답변:  shine언어적 답변:  on행동적 답변:  a언어적 답변:  new행동적 답변:  flashlight언어적 답변:  meanings행동적 답변:  through언어적 답변:  or행동적 답변:  the언어적 답변:  con행동적 답변:  colored언어적 답변: notations행동적 답변:  water언어적 답변:  through행동적 답변:  from언어적 답변:  use행동적 답변:  different언어적 답변: .\n",
      "\n",
      "행동적 답변:  angles언어적 답변: 3행동적 답변:  (언어적 답변: .행동적 답변: e언어적 답변:  **행동적 답변: .g언어적 답변: L행동적 답변: .,언어적 답변: ingu행동적 답변:  directly언어적 답변: istic행동적 답변:  on언어적 답변:  Construction행동적 답변:  top언어적 답변: **:행동적 답변: ,언어적 답변:  The행동적 답변:  at언어적 답변:  question행동적 답변:  an언어적 답변:  \"행동적 답변:  angle언어적 답변: Why행동적 답변: ,언어적 답변:  is행동적 답변:  or언어적 답변:  the행동적 답변:  horizontally언어적 답변:  sky행동적 답변: ).언어적 답변:  blue행동적 답변:  Ob언어적 답변: ?\"행동적 답변: serve언어적 답변:  follows행동적 답변:  how언어적 답변:  a행동적 답변:  the언어적 답변:  classic행동적 답변:  light언어적 답변:  pattern행동적 답변:  behaves언어적 답변:  in행동적 답변: .\n",
      "\n",
      "언어적 답변:  English행동적 답변: **언어적 답변: :행동적 답변: Step언어적 답변:  interrog행동적 답변:  언어적 답변: ative행동적 답변: 4언어적 답변:  constructions행동적 답변: :언어적 답변:  with행동적 답변:  Consider언어적 답변:  auxiliary행동적 답변:  Additional언어적 답변:  verbs행동적 답변:  Factors언어적 답변:  like행동적 답변: **\n",
      "\n",
      "언어적 답변:  \"행동적 답변: *언어적 답변: is행동적 답변:  Think언어적 답변: ,\"행동적 답변:  about언어적 답변:  which행동적 답변:  other언어적 답변:  indicates행동적 답변:  factors언어적 답변:  a행동적 답변:  that언어적 답변:  state행동적 답변:  might언어적 답변:  of행동적 답변:  affect언어적 답변:  being행동적 답변:  the언어적 답변: .행동적 답변:  apparent언어적 답변:  The행동적 답변:  color언어적 답변:  structure행동적 답변:  of언어적 답변:  of행동적 답변:  the언어적 답변:  this행동적 답변:  sky언어적 답변:  sentence행동적 답변: .언어적 답변:  also행동적 답변:  For언어적 답변:  employs행동적 답변:  example언어적 답변:  a행동적 답변: :\n",
      "언어적 답변:  pre행동적 답변:  +언어적 답변: pos행동적 답변:  Air언어적 답변: itional행동적 답변:  pollution언어적 답변:  phrase행동적 답변:  and언어적 답변:  \"행동적 답변:  dust언어적 답변: in행동적 답변:  can언어적 답변:  the행동적 답변:  scatter언어적 답변:  sky행동적 답변:  light언어적 답변: \"행동적 답변:  in언어적 답변:  to행동적 답변:  different언어적 답변:  specify행동적 답변:  ways언어적 답변:  the행동적 답변: .\n",
      "언어적 답변:  location행동적 답변:  +언어적 답변: ,행동적 답변:  Atmospheric언어적 답변:  showing행동적 답변:  conditions언어적 답변:  an행동적 답변:  like언어적 답변:  understanding행동적 답변:  humidity언어적 답변:  of행동적 답변: ,언어적 답변:  spatial행동적 답변:  temperature언어적 답변:  relationships행동적 답변: ,언어적 답변: .\n",
      "\n",
      "행동적 답변:  and언어적 답변: 4행동적 답변:  air언어적 답변: .행동적 답변:  pressure언어적 답변:  **행동적 답변:  can언어적 답변: Sy행동적 답변:  also언어적 답변: nt행동적 답변:  impact언어적 답변: actic행동적 답변:  the언어적 답변:  Analysis행동적 답변:  way언어적 답변: **:행동적 답변:  we언어적 답변:  In행동적 답변:  perceive언어적 답변:  lingu행동적 답변:  colors언어적 답변: istics행동적 답변: .\n",
      "언어적 답변: ,행동적 답변:  +언어적 답변:  questions행동적 답변:  The언어적 답변:  often행동적 답변:  Earth언어적 답변:  follow행동적 답변: 's언어적 답변:  specific행동적 답변:  rotation언어적 답변:  gramm행동적 답변:  and언어적 답변: atical행동적 답변:  tilt언어적 답변:  patterns행동적 답변:  also언어적 답변: ,행동적 답변:  influence언어적 답변:  such행동적 답변:  the언어적 답변:  as행동적 답변:  angle언어적 답변:  interrog행동적 답변:  at언어적 답변: ative행동적 답변:  which언어적 답변:  constructions행동적 답변:  sunlight언어적 답변:  (행동적 답변:  enters언어적 답변: e행동적 답변:  our언어적 답변: .g행동적 답변:  atmosphere언어적 답변: .,행동적 답변: .\n",
      "\n",
      "언어적 답변:  Who행동적 답변: **언어적 답변:  did행동적 답변: Step언어적 답변:  it행동적 답변:  언어적 답변: ?,행동적 답변: 5언어적 답변:  What행동적 답변: :언어적 답변: 's행동적 답변:  Join언어적 답변:  happening행동적 답변:  a언어적 답변: ?),행동적 답변:  Community언어적 답변:  which행동적 답변:  or언어적 답변:  are행동적 답변:  Take언어적 답변:  governed행동적 답변:  a언어적 답변:  by행동적 답변:  Class언어적 답변:  rules행동적 답변: **\n",
      "\n",
      "언어적 답변:  and행동적 답변: *언어적 답변:  conventions행동적 답변:  If언어적 답변: .행동적 답변:  you언어적 답변:  The행동적 답변: 're언어적 답변:  question행동적 답변:  really언어적 답변:  \"행동적 답변:  interested언어적 답변: Why행동적 답변:  in언어적 답변:  is행동적 답변:  learning언어적 답변:  the행동적 답변:  more언어적 답변:  sky행동적 답변:  about언어적 답변:  blue행동적 답변:  this언어적 답변: ?\"행동적 답변:  topic언어적 답변:  adher행동적 답변: ,언어적 답변: es행동적 답변:  consider언어적 답변:  to행동적 답변:  joining언어적 답변:  this행동적 답변:  an언어적 답변:  pattern행동적 답변:  online언어적 답변: ,행동적 답변:  community언어적 답변:  using행동적 답변:  (언어적 답변:  a행동적 답변: e언어적 답변:  combination행동적 답변: .g언어적 답변:  of행동적 답변: .,언어적 답변:  auxiliary행동적 답변:  Reddit언어적 답변:  verbs행동적 답변: 's언어적 답변:  (\"행동적 답변:  r언어적 답변: is행동적 답변: /언어적 답변: \")행동적 답변: Ask언어적 답변:  and행동적 답변: Science언어적 답변:  phrases행동적 답변: )언어적 답변:  (\"행동적 답변:  or언어적 답변: Why행동적 답변:  taking언어적 답변: ...행동적 답변:  a언어적 답변: \")행동적 답변:  class언어적 답변:  to행동적 답변:  to언어적 답변:  create행동적 답변:  learn언어적 답변:  a행동적 답변:  more언어적 답변:  clear행동적 답변:  about언어적 답변:  inquiry행동적 답변:  physics언어적 답변: .\n",
      "\n",
      "행동적 답변:  and언어적 답변: 5행동적 답변:  astronomy언어적 답변: .행동적 답변: .\n",
      "언어적 답변:  **행동적 답변: *언어적 답변: Semantic행동적 답변:  Discuss언어적 답변:  Fields행동적 답변: ing언어적 답변: **:행동적 답변:  the언어적 답변:  The행동적 답변:  subject언어적 답변:  concept행동적 답변:  with언어적 답변:  of행동적 답변:  experts언어적 답변:  color행동적 답변:  and언어적 답변:  can행동적 답변:  enthusiasts언어적 답변:  be행동적 답변:  can언어적 답변:  analyzed행동적 답변:  help언어적 답변:  through행동적 답변:  deepen언어적 답변:  semantic행동적 답변:  your언어적 답변:  fields행동적 답변:  understanding언어적 답변: ,행동적 답변: .\n",
      "\n",
      "언어적 답변:  which행동적 답변: **언어적 답변:  group행동적 답변: Bonus언어적 답변:  related행동적 답변:  Step언어적 답변:  concepts행동적 답변: :언어적 답변: ,행동적 답변:  Explore언어적 답변:  meanings행동적 답변:  Art언어적 답변: ,행동적 답변: istic언어적 답변:  or행동적 답변:  Express언어적 답변:  ideas행동적 답변: ions언어적 답변:  together행동적 답변: **\n",
      "\n",
      "언어적 답변: .행동적 답변: *언어적 답변:  In행동적 답변:  Take언어적 답변:  this행동적 답변:  inspiration언어적 답변:  case행동적 답변:  from언어적 답변: ,행동적 답변:  the언어적 답변:  \"행동적 답변:  blue언어적 답변: blue행동적 답변:  color언어적 답변: \"행동적 답변:  of언어적 답변:  belongs행동적 답변:  the언어적 답변:  to행동적 답변:  sky언어적 답변:  the행동적 답변:  and언어적 답변:  semantic행동적 답변:  create언어적 답변:  field행동적 답변:  some언어적 답변:  of행동적 답변:  art언어적 답변:  colors행동적 답변: !언어적 답변: ,행동적 답변:  Draw언어적 답변:  with행동적 답변: ,언어적 답변:  associated행동적 답변:  paint언어적 답변:  concepts행동적 답변: ,언어적 답변:  like행동적 답변:  write언어적 답변:  hues행동적 답변:  poetry언어적 답변: ,행동적 답변: ,언어적 답변:  shades행동적 답변:  or언어적 답변: ,행동적 답변:  compose언어적 답변:  and행동적 답변:  music언어적 답변:  lighting행동적 답변:  inspired언어적 답변:  effects행동적 답변:  by언어적 답변: .\n",
      "\n",
      "행동적 답변:  this언어적 답변: 6행동적 답변:  beautiful언어적 답변: .행동적 답변:  phenomenon언어적 답변:  **행동적 답변: .\n",
      "\n",
      "언어적 답변: L행동적 답변: By언어적 답변: ingu행동적 답변:  following언어적 답변: istic행동적 답변:  these언어적 답변:  Univers행동적 답변:  steps언어적 답변: als행동적 답변: ,언어적 답변: **:행동적 답변:  you언어적 답변:  Despite행동적 답변: 'll언어적 답변:  cultural행동적 답변:  gain언어적 답변:  variations행동적 답변:  a언어적 답변:  in행동적 답변:  better언어적 답변:  language행동적 답변:  understanding언어적 답변:  usage행동적 답변:  of언어적 답변: ,행동적 답변:  why언어적 답변:  certain행동적 답변:  the언어적 답변:  linguistic행동적 답변:  sky언어적 답변:  structures행동적 답변:  appears언어적 답변:  (행동적 답변:  blue언어적 답변: like행동적 답변:  and언어적 답변:  the행동적 답변:  develop언어적 답변:  question행동적 답변:  a언어적 답변: -con행동적 답변:  deeper언어적 답변: struction행동적 답변:  appreciation언어적 답변: )행동적 답변:  for언어적 답변:  can행동적 답변:  the언어적 답변:  be행동적 답변:  science언어적 답변:  considered행동적 답변:  behind언어적 답변:  universal행동적 답변:  our언어적 답변:  across행동적 답변:  natural언어적 답변:  languages행동적 답변:  world언어적 답변: ,행동적 답변: .언어적 답변:  indicating언어적 답변:  a행동적 답변: 언어적 답변:  shared언어적 답변:  cognitive언어적 답변:  basis언어적 답변:  for언어적 답변:  human언어적 답변:  communication언어적 답변: .\n",
      "\n",
      "언어적 답변: 7언어적 답변: .언어적 답변:  **언어적 답변: Pr언어적 답변: ag언어적 답변: matic언어적 답변:  Analysis언어적 답변: **:언어적 답변:  The언어적 답변:  context언어적 답변:  of언어적 답변:  \"언어적 답변: Why언어적 답변:  is언어적 답변:  the언어적 답변:  sky언어적 답변:  blue언어적 답변: ?\"언어적 답변:  reveals언어적 답변:  an언어적 답변:  underlying언어적 답변:  pragmatic언어적 답변:  intention언어적 답변: :언어적 답변:  to언어적 답변:  seek언어적 답변:  an언어적 답변:  explanation언어적 답변:  or언어적 답변:  understanding언어적 답변:  behind언어적 답변:  this언어적 답변:  observation언어적 답변: .언어적 답변:  This언어적 답변:  implies언어적 답변:  that언어적 답변:  the언어적 답변:  speaker언어적 답변:  is언어적 답변:  aware언어적 답변:  of언어적 답변:  their언어적 답변:  own언어적 답변:  knowledge언어적 답변:  limitations언어적 답변:  and언어적 답변:  seeks언어적 답변:  additional언어적 답변:  information언어적 답변:  from언어적 답변:  others언어적 답변: .\n",
      "\n",
      "언어적 답변: In언어적 답변:  summary언어적 답변: ,언어적 답변:  the언어적 답변:  question언어적 답변:  \"언어적 답변: Why언어적 답변:  is언어적 답변:  the언어적 답변:  sky언어적 답변:  blue언어적 답변: ?\"언어적 답변:  involves언어적 답변:  a언어적 답변:  complex언어적 답변:  inter언어적 답변: play언어적 답변:  of언어적 답변:  linguistic언어적 답변:  structures언어적 답변:  (언어적 답변: inter언어적 답변: rog언어적 답변: ative언어적 답변:  constructions언어적 답변: ),언어적 답변:  semantic언어적 답변:  meanings언어적 답변:  (언어적 답변: color언어적 답변:  terms언어적 답변: ),언어적 답변:  lexical언어적 답변: ization언어적 답변:  (언어적 답변: word언어적 답변:  usage언어적 답변:  and언어적 답변:  evolution언어적 답변: ),언어적 답변:  and언어적 답변:  pragmatic언어적 답변:  intentions언어적 답변:  (언어적 답변: seek언어적 답변: ing언어적 답변:  explanation언어적 답변: ).언어적 답변:  By언어적 답변:  examining언어적 답변:  these언어적 답변:  aspects언어적 답변: ,언어적 답변:  we언어적 답변:  can언어적 답변:  gain언어적 답변:  insight언어적 답변:  into언어적 답변:  how언어적 답변:  language언어적 답변:  reflects언어적 답변:  our언어적 답변:  understanding언어적 답변:  of언어적 답변:  the언어적 답변:  world언어적 답변:  and언어적 답변:  ourselves언어적 답변: .\n",
      "\n",
      "언어적 답변: Now언어적 답변: ,언어적 답변:  to언어적 답변:  answer언어적 답변:  this언어적 답변:  question언어적 답변:  directly언어적 답변: :언어적 답변:  The언어적 답변:  sky언어적 답변:  appears언어적 답변:  blue언어적 답변:  because언어적 답변:  of언어적 답변:  a언어적 답변:  phenomenon언어적 답변:  called언어적 답변:  Ray언어적 답변: leigh언어적 답변:  scattering언어적 답변: ,언어적 답변:  which언어적 답변:  is언어적 답변:  the언어적 답변:  scattering언어적 답변:  of언어적 답변:  light언어적 답변:  by언어적 답변:  small언어적 답변:  particles언어적 답변:  or언어적 답변:  molecules언어적 답변:  in언어적 답변:  the언어적 답변:  Earth언어적 답변: 's언어적 답변:  atmosphere언어적 답변: .언어적 답변:  When언어적 답변:  sunlight언어적 답변:  enters언어적 답변:  the언어적 답변:  atmosphere언어적 답변: ,언어적 답변:  shorter언어적 답변:  (언어적 답변: blue언어적 답변: )언어적 답변:  wavelengths언어적 답변:  are언어적 답변:  scattered언어적 답변:  more언어적 답변:  than언어적 답변:  longer언어적 답변:  wavelengths언어적 답변:  due언어적 답변:  to언어적 답변:  their언어적 답변:  smaller언어적 답변:  size언어적 답변:  relative언어적 답변:  to언어적 답변:  the언어적 답변:  particle언어적 답변:  diameter언어적 답변: ,언어적 답변:  resulting언어적 답변:  in언어적 답변:  the언어적 답변:  blue언어적 답변:  hue언어적 답변:  we언어적 답변:  perceive언어적 답변:  as언어적 답변:  the언어적 답변:  sky언어적 답변: .언어적 답변: "
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import ollama\n",
    "\n",
    "def handle_streaming_response(prefix, response_stream):\n",
    "    \"\"\"\n",
    "    스트리밍 응답을 실시간으로 출력하는 함수.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for chunk in response_stream:\n",
    "            if 'content' in chunk['message']:\n",
    "                print(f\"{prefix}: {chunk['message']['content']}\", end='', flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"{prefix}: 에러 발생 - {e}\")\n",
    "\n",
    "def get_linguistic_response(model, user_input):\n",
    "    \"\"\"\n",
    "    언어적 답변을 얻기 위한 함수.\n",
    "    \"\"\"\n",
    "    prompt = f\"Provide a detailed linguistic explanation for the following question:\\n{user_input}\"\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ],\n",
    "        stream=True  # 스트리밍 활성화\n",
    "    )\n",
    "    handle_streaming_response(\"언어적 답변\", response)\n",
    "\n",
    "def get_action_response(model, user_input):\n",
    "    \"\"\"\n",
    "    행동적 답변을 얻기 위한 함수.\n",
    "    \"\"\"\n",
    "    prompt = f\"Provide actionable steps based on the following question:\\n{user_input}\"\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ],\n",
    "        stream=True  # 스트리밍 활성화\n",
    "    )\n",
    "    handle_streaming_response(\"행동적 답변\", response)\n",
    "\n",
    "def main():\n",
    "    user_input = \"Why is the sky blue?\"\n",
    "    model = 'llama3.2:latest'\n",
    "\n",
    "    # 두 개의 스레드를 생성하여 병렬로 실행\n",
    "    thread1 = threading.Thread(target=get_linguistic_response, args=(model, user_input))\n",
    "    thread2 = threading.Thread(target=get_action_response, args=(model, user_input))\n",
    "\n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "\n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in ./.pyenv/versions/3.9.13/lib/python3.9/site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nest_asyncio\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
